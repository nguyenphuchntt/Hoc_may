{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:54:53.49443Z",
     "iopub.status.busy": "2025-12-03T17:54:53.494177Z",
     "iopub.status.idle": "2025-12-03T17:54:56.856291Z",
     "shell.execute_reply": "2025-12-03T17:54:56.854683Z"
    },
    "papermill": {
     "duration": 3.368626,
     "end_time": "2025-12-03T17:54:56.858355",
     "exception": false,
     "start_time": "2025-12-03T17:54:53.489729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Global Configuration\n",
    "validate_or_submit = 'submit'\n",
    "verbose = True\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "import itertools\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator, clone\n",
    "from sklearn.model_selection import cross_val_predict, GroupKFold, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "from scipy.ndimage import binary_closing, binary_opening, gaussian_filter1d, median_filter\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "\n",
    "SEED=1234\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "rnd = np.random.RandomState(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def get_gpu_params():\n",
    "    \"\"\"Detect GPU and return XGBoost parameters.\"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True)\n",
    "        if result.returncode == 0:\n",
    "            print('✓ GPU detected - using GPU acceleration')\n",
    "            return {'tree_method': 'hist', 'device': 'cuda', 'predictor': 'gpu_predictor'}\n",
    "    except:\n",
    "        pass\n",
    "    print('⚠ No GPU - using CPU')\n",
    "    return {'tree_method': 'hist', 'predictor': 'cpu_predictor'}\n",
    "\n",
    "GPU_PARAMS = get_gpu_params()\n",
    "class CFG:\n",
    "    BASE_PATH = \"/kaggle/input/MABe-mouse-behavior-detection\"\n",
    "    train_path = f\"{BASE_PATH}/train.csv\"\n",
    "    test_path = f\"{BASE_PATH}/test.csv\"\n",
    "    train_annotation_path = f\"{BASE_PATH}/train_annotation\"\n",
    "    train_tracking_path = f\"{BASE_PATH}/train_tracking\"\n",
    "    test_tracking_path = f\"{BASE_PATH}/test_tracking\"\n",
    "\n",
    "    model_path = \"/kaggle/input/social-action-recognition-in-mice-xgb-catboost\"\n",
    "    model_name = \"xgboost\"\n",
    "    MODEL_SAVE_PATH = \"/kaggle/working/models\"\n",
    "    \n",
    "    # mode = \"validate\"\n",
    "    mode = \"submit\"\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        verbosity=0, \n",
    "        random_state=42,\n",
    "        n_estimators=700, \n",
    "        learning_rate=0.05, \n",
    "        max_depth=6,\n",
    "        min_child_weight=5, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8,\n",
    "        **GPU_PARAMS  # GPU acceleration\n",
    "    )\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Cấu hình cho CatBoost\n",
    "CAT_PARAMS = {\n",
    "    'iterations': 700,         # Tương đương n_estimators\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 7,\n",
    "    'loss_function': 'Logloss',\n",
    "    'verbose': 0,\n",
    "    'random_seed': 42,\n",
    "    'task_type': 'GPU',        # Dùng GPU để train nhanh\n",
    "    'devices': '0',\n",
    "    'allow_writing_files': False\n",
    "}\n",
    "\n",
    "class DataSampler(ClassifierMixin, BaseEstimator):\n",
    "    def __init__(self, estimator, neg_pos_ratio=10.0): \n",
    "        self.estimator = estimator\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_arr = np.asarray(X)\n",
    "        y_arr = np.assarray(y)\n",
    "        \n",
    "        pos_indices = np.where(y_arr == 1)[0]\n",
    "        neg_indices = np.where(y_arr == 0)[0]\n",
    "        \n",
    "        if len(pos_indices) == 0:\n",
    "            self.estimator.fit(X_arr[::10], y_arr[::10])\n",
    "            self.classes_ = self.estimator.classes_\n",
    "            return self\n",
    "\n",
    "        n_neg_keep = int(len(pos_indices) * self.neg_pos_ratio)\n",
    "        \n",
    "        if len(neg_indices) > n_neg_keep:\n",
    "            kept_neg_indices = np.random.choice(neg_indices, n_neg_keep, replace=False)\n",
    "        else:\n",
    "            kept_neg_indices = neg_indices\n",
    "            \n",
    "        final_indices = np.concatenate([pos_indices, kept_neg_indices])\n",
    "        np.random.shuffle(final_indices)\n",
    "        \n",
    "        self.estimator.fit(X_arr[final_indices], y_arr[final_indices])\n",
    "        self.classes_ = self.estimator.classes_\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(np.array(X))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(np.array(X))\n",
    "\n",
    "def train_and_save_models(body_parts_tracked_str, switch_tr, X_tr, label, meta, section_id):\n",
    "    \"\"\"Train XGBoost and CatBoost models and save them to disk.\"\"\"\n",
    "    import xgboost\n",
    "    import catboost\n",
    "    \n",
    "    # Create save directory\n",
    "    save_dir = os.path.join(CFG.MODEL_SAVE_PATH, switch_tr, f\"section_{section_id}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    model_list = []\n",
    "    \n",
    "    for action in label.columns:\n",
    "        action_mask = ~label[action].isna().values\n",
    "        y_action = label[action][action_mask].values.astype(int)\n",
    "        \n",
    "        if not (y_action == 0).all() and len(np.unique(y_action)) >= 2:\n",
    "            print(f\"  Training models for action: {action}\")\n",
    "            \n",
    "            # Train XGBoost\n",
    "            model_xgb = DataSampler(clone(CFG.model), neg_pos_ratio=10.0)\n",
    "            model_xgb.fit(X_tr[action_mask], y_action)\n",
    "            \n",
    "            # Train CatBoost\n",
    "            cat_model = CatBoostClassifier(**CAT_PARAMS)\n",
    "            model_cat = DataSampler(cat_model, neg_pos_ratio=10.0)\n",
    "            model_cat.fit(X_tr[action_mask], y_action)\n",
    "            \n",
    "            # Save XGBoost model\n",
    "            xgb_path = os.path.join(save_dir, f\"{action}_xgboost.joblib\")\n",
    "            joblib.dump(model_xgb, xgb_path)\n",
    "            print(f\"    Saved XGBoost: {xgb_path}\")\n",
    "            \n",
    "            cat_path = os.path.join(save_dir, f\"{action}_catboost.cbm\")\n",
    "            model_cat.estimator.save_model(cat_path)\n",
    "            joblib.dump(model_cat, os.path.join(save_dir, f\"{action}_catboost_wrapper.joblib\"))\n",
    "            print(f\"    Saved CatBoost: {cat_path}\")\n",
    "            \n",
    "            model_list.append(action)\n",
    "    \n",
    "    metadata = {\n",
    "        \"feature_names\": list(X_tr.columns),\n",
    "        \"actions\": model_list,\n",
    "        \"body_parts_tracked_str\": body_parts_tracked_str,\n",
    "        \"switch_type\": switch_tr,\n",
    "        \"xgboost_version\": xgboost.__version__,\n",
    "        \"catboost_version\": catboost.__version__\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(save_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"  Saved metadata: {metadata_path}\")\n",
    "    \n",
    "    return model_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:54:56.866656Z",
     "iopub.status.busy": "2025-12-03T17:54:56.865979Z",
     "iopub.status.idle": "2025-12-03T17:54:56.873778Z",
     "shell.execute_reply": "2025-12-03T17:54:56.873255Z"
    },
    "papermill": {
     "duration": 0.012179,
     "end_time": "2025-12-03T17:54:56.874881",
     "exception": false,
     "start_time": "2025-12-03T17:54:56.862702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_sqrt(x):\n",
    "    \"\"\"Robust sqrt that clips negative values to 0.\"\"\"\n",
    "    if isinstance(x, (pd.Series, pd.DataFrame)):\n",
    "        return np.sqrt(x.clip(lower=0))\n",
    "    return np.sqrt(np.maximum(x, 0))\n",
    "\n",
    "# Suppress runtime warnings for invalid values in comparisons\n",
    "warnings.filterwarnings('ignore', 'invalid value encountered')\n",
    "\n",
    "def smooth_coordinates(df, sigma=1.5):\n",
    "    \"\"\"Apply Gaussian smoothing to coordinate columns.\"\"\"\n",
    "    df_smooth = df.copy()\n",
    "    for col in df.columns:\n",
    "        # Fill NaN before smoothing\n",
    "        filled = df[col].interpolate(method='linear', limit_direction='both').fillna(0).values\n",
    "        # Apply Gaussian filter\n",
    "        smoothed = gaussian_filter1d(filled, sigma=sigma)\n",
    "        df_smooth[col] = smoothed\n",
    "    return df_smooth\n",
    "\n",
    "\n",
    "def safe_rolling(series, window, func, min_periods=None):\n",
    "    \"\"\"Safe rolling operation with NaN handling\"\"\"\n",
    "    if min_periods is None:\n",
    "        min_periods = max(1, window // 4)\n",
    "    return series.rolling(window, min_periods=min_periods, center=True).apply(func, raw=True)\n",
    "\n",
    "def _scale(n_frames_at_30fps, fps, ref=30.0):\n",
    "    \"\"\"Scale a frame count defined at 30 fps to the current video's fps.\"\"\"\n",
    "    return max(1, int(round(n_frames_at_30fps * float(fps) / ref)))\n",
    "\n",
    "def _scale_signed(n_frames_at_30fps, fps, ref=30.0):\n",
    "    \"\"\"Signed version of _scale for forward/backward shifts (keeps at least 1 frame when |n|>=1).\"\"\"\n",
    "    if n_frames_at_30fps == 0:\n",
    "        return 0\n",
    "    s = 1 if n_frames_at_30fps > 0 else -1\n",
    "    mag = max(1, int(round(abs(n_frames_at_30fps) * float(fps) / ref)))\n",
    "    return s * mag\n",
    "\n",
    "def _fps_from_meta(meta_df, fallback_lookup, default_fps=30.0):\n",
    "    if 'frames_per_second' in meta_df.columns and pd.notnull(meta_df['frames_per_second']).any():\n",
    "        return float(meta_df['frames_per_second'].iloc[0])\n",
    "    vid = meta_df['video_id'].iloc[0]\n",
    "    return float(fallback_lookup.get(vid, default_fps))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:54:56.880857Z",
     "iopub.status.busy": "2025-12-03T17:54:56.880664Z",
     "iopub.status.idle": "2025-12-03T17:54:56.919668Z",
     "shell.execute_reply": "2025-12-03T17:54:56.919166Z"
    },
    "papermill": {
     "duration": 0.043318,
     "end_time": "2025-12-03T17:54:56.920664",
     "exception": false,
     "start_time": "2025-12-03T17:54:56.877346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "import itertools\n",
    "\n",
    "def safe_sqrt(x):\n",
    "    \"\"\"Robust sqrt that clips negative values to 0.\"\"\"\n",
    "    if isinstance(x, (pd.Series, pd.DataFrame)):\n",
    "        return np.sqrt(x.clip(lower=0))\n",
    "    return np.sqrt(np.maximum(x, 0))\n",
    "\n",
    "def _scale(n_frames_at_30fps, fps, ref=30.0):\n",
    "    \"\"\"Scale a frame count defined at 30 fps to the current video's fps.\"\"\"\n",
    "    return max(1, int(round(n_frames_at_30fps * float(fps) / ref)))\n",
    "\n",
    "def _scale_signed(n_frames_at_30fps, fps, ref=30.0):\n",
    "    \"\"\"Signed version of _scale for forward/backward shifts (keeps at least 1 frame when |n|>=1).\"\"\"\n",
    "    if n_frames_at_30fps == 0:\n",
    "        return 0\n",
    "    s = 1 if n_frames_at_30fps > 0 else -1\n",
    "    mag = max(1, int(round(abs(n_frames_at_30fps) * float(fps) / ref)))\n",
    "    return s * mag\n",
    "\n",
    "def add_curvature_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Trajectory curvature (window lengths scaled by fps).\"\"\"\n",
    "    vel_x = center_x.diff()\n",
    "    vel_y = center_y.diff()\n",
    "    acc_x = vel_x.diff()\n",
    "    acc_y = vel_y.diff()\n",
    "\n",
    "    cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "    vel_mag = safe_sqrt(vel_x**2 + vel_y**2)\n",
    "    curvature = np.abs(cross_prod) / (vel_mag**3 + 1e-6)  # invariant to time scaling\n",
    "\n",
    "    new_features = {}\n",
    "    for w in [30, 60]:\n",
    "        ws = _scale(w, fps)\n",
    "        new_features[f'curv_mean_{w}'] = curvature.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    angle = np.arctan2(vel_y, vel_x)\n",
    "    angle_change = np.abs(angle.diff())\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    new_features[f'turn_rate_{w}'] = angle_change.rolling(ws, min_periods=max(1, ws // 6)).sum()\n",
    "\n",
    "    if new_features:\n",
    "        X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_multiscale_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Multi-scale temporal features (speed in cm/s; windows scaled by fps).\"\"\"\n",
    "    # displacement per frame is already in cm (pix normalized earlier); convert to cm/s\n",
    "    speed = safe_sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    \n",
    "    # Smooth speed signal using Savitzky-Golay filter (window ~15 frames)\n",
    "    try:\n",
    "        ws = min(15, len(speed) // 2)\n",
    "        if ws >= 5 and ws % 2 == 1:  # Must be odd\n",
    "            speed_smooth = pd.Series(savgol_filter(speed.fillna(0).values, ws, 3), index=speed.index)\n",
    "            X['speed_smooth'] = speed_smooth\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    new_features = {}\n",
    "    scales = [10, 40, 160]\n",
    "    for scale in scales:\n",
    "        ws = _scale(scale, fps)\n",
    "        if len(speed) >= ws:\n",
    "            new_features[f'sp_m{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).mean()\n",
    "            new_features[f'sp_s{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).var().clip(lower=0).pow(0.5)\n",
    "\n",
    "    if len(scales) >= 2 and f'sp_m{scales[0]}' in new_features and f'sp_m{scales[-1]}' in new_features:\n",
    "        new_features['sp_ratio'] = new_features[f'sp_m{scales[0]}'] / (new_features[f'sp_m{scales[-1]}'] + 1e-6)\n",
    "\n",
    "    if new_features:\n",
    "        X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_state_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Behavioral state transitions; bins adjusted so semantics are fps-invariant.\"\"\"\n",
    "    speed = safe_sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)  # cm/s\n",
    "    w_ma = _scale(15, fps)\n",
    "    speed_ma = speed.rolling(w_ma, min_periods=max(1, w_ma // 3)).mean()\n",
    "\n",
    "    try:\n",
    "        # Original bins (cm/frame): [-inf, 0.5, 2.0, 5.0, inf]\n",
    "        # Convert to cm/s by multiplying by fps to keep thresholds consistent across fps.\n",
    "        bins = [-np.inf, 0.5 * fps, 2.0 * fps, 5.0 * fps, np.inf]\n",
    "        speed_states = pd.cut(speed_ma, bins=bins, labels=[0, 1, 2, 3]).astype(float)\n",
    "\n",
    "        new_features = {}\n",
    "        for window in [60, 120]:\n",
    "            ws = _scale(window, fps)\n",
    "            if len(speed_states) >= ws:\n",
    "                for state in [0, 1, 2, 3]:\n",
    "                    new_features[f's{state}_{window}'] = (\n",
    "                        (speed_states == state).astype(float)\n",
    "                        .rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "                    )\n",
    "                state_changes = (speed_states != speed_states.shift(1)).astype(float)\n",
    "                new_features[f'trans_{window}'] = state_changes.rolling(ws, min_periods=max(1, ws // 6)).sum()\n",
    "        \n",
    "        if new_features:\n",
    "            X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_longrange_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Long-range temporal features (windows & spans scaled by fps).\"\"\"\n",
    "    new_features = {}\n",
    "    for window in [120, 240]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(center_x) >= ws:\n",
    "            new_features[f'x_ml{window}'] = center_x.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "            new_features[f'y_ml{window}'] = center_y.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "\n",
    "    # EWM spans also interpreted in frames\n",
    "    for span in [60, 120]:\n",
    "        s = _scale(span, fps)\n",
    "        new_features[f'x_e{span}'] = center_x.ewm(span=s, min_periods=1).mean()\n",
    "        new_features[f'y_e{span}'] = center_y.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "    speed = safe_sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)  # cm/s\n",
    "    for window in [60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(speed) >= ws:\n",
    "            new_features[f'sp_pct{window}'] = speed.rolling(ws, min_periods=max(5, ws // 6)).rank(pct=True)\n",
    "\n",
    "    if new_features:\n",
    "        X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_interaction_features(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    \"\"\"Social interaction features (windows scaled by fps).\"\"\"\n",
    "    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n",
    "        return X\n",
    "\n",
    "    rel_x = mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x']\n",
    "    rel_y = mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y']\n",
    "    rel_dist = safe_sqrt(rel_x**2 + rel_y**2)\n",
    "\n",
    "    # per-frame velocities (cm/frame)\n",
    "    A_vx = mouse_pair['A']['body_center']['x'].diff()\n",
    "    A_vy = mouse_pair['A']['body_center']['y'].diff()\n",
    "    B_vx = mouse_pair['B']['body_center']['x'].diff()\n",
    "    B_vy = mouse_pair['B']['body_center']['y'].diff()\n",
    "\n",
    "    A_lead = (A_vx * rel_x + A_vy * rel_y) / (safe_sqrt(A_vx**2 + A_vy**2) * rel_dist + 1e-6)\n",
    "    B_lead = (B_vx * (-rel_x) + B_vy * (-rel_y)) / (safe_sqrt(B_vx**2 + B_vy**2) * rel_dist + 1e-6)\n",
    "\n",
    "    new_features = {}\n",
    "    for window in [30, 60]:\n",
    "        ws = _scale(window, fps)\n",
    "        new_features[f'A_ld{window}'] = A_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "        new_features[f'B_ld{window}'] = B_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    approach = -rel_dist.diff()  # decreasing distance => positive approach\n",
    "    chase = approach * B_lead\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    new_features[f'chase_{w}'] = chase.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    for window in [60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        A_sp = safe_sqrt(A_vx**2 + A_vy**2)\n",
    "        B_sp = safe_sqrt(B_vx**2 + B_vy**2)\n",
    "        new_features[f'sp_cor{window}'] = A_sp.rolling(ws, min_periods=max(1, ws // 6)).corr(B_sp)\n",
    "\n",
    "    if new_features:\n",
    "        X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_fft_features(X, signal, signal_name, fps, window_size=120):\n",
    "    \"\"\"\n",
    "    Extract FFT-based frequency domain features using Vectorized Spectrogram.\n",
    "    Massively faster than rolling loop.\n",
    "    \"\"\"\n",
    "    from scipy.signal import spectrogram\n",
    "    \n",
    "    ws = _scale(window_size, fps)\n",
    "    N = len(signal)\n",
    "    \n",
    "    if N < ws:\n",
    "        return X\n",
    "    \n",
    "    try:\n",
    "        # Fill NaNs for FFT input\n",
    "        sig_filled = signal.ffill().bfill().fillna(0).values\n",
    "        \n",
    "        # Compute Spectrogram\n",
    "        # nperseg=ws, noverlap=ws-1 gives stride=1, mimicking rolling(window=ws)\n",
    "        f, t, Sxx = spectrogram(sig_filled, fs=fps, window='hann', \n",
    "                                nperseg=ws, noverlap=ws-1, \n",
    "                                mode='psd', detrend='constant', scaling='density')\n",
    "        \n",
    "        # Sxx shape: (n_freqs, n_time_steps)\n",
    "        # Transpose to (n_time_steps, n_freqs) to align with time\n",
    "        Sxx = Sxx.T + 1e-20 # Avoid log(0)\n",
    "        \n",
    "        # 1. Spectral Energy\n",
    "        spectral_energy = np.sum(Sxx, axis=1)\n",
    "        \n",
    "        # 2. Spectral Entropy\n",
    "        # Normalize to probability distribution along freq axis\n",
    "        P_norm = Sxx / (spectral_energy[:, None])\n",
    "        spectral_entropy = -np.sum(P_norm * np.log2(P_norm), axis=1)\n",
    "        \n",
    "        # 3. Dominant Frequency\n",
    "        # Skip DC component (index 0)\n",
    "        dom_idx = np.argmax(Sxx[:, 1:], axis=1) + 1\n",
    "        dominant_freq = f[dom_idx]\n",
    "        \n",
    "        # 4. Low Band Power (0-1 Hz)\n",
    "        mask_low = (f >= 0) & (f < 1.0)\n",
    "        power_low = np.sum(Sxx[:, mask_low], axis=1)\n",
    "        \n",
    "        # 5. High Band Power (1-5 Hz)\n",
    "        mask_high = (f >= 1.0) & (f < 5.0)\n",
    "        power_high = np.sum(Sxx[:, mask_high], axis=1)\n",
    "        \n",
    "        # Padding to match original length N\n",
    "        # The spectrogram output length is roughly N - ws + 1\n",
    "        pad_head = ws // 2\n",
    "        \n",
    "        feats_dict = {\n",
    "            f'{signal_name}_fft_domfreq': dominant_freq,\n",
    "            f'{signal_name}_fft_energy': spectral_energy,\n",
    "            f'{signal_name}_fft_entropy': spectral_entropy,\n",
    "            f'{signal_name}_fft_power_low': power_low,\n",
    "            f'{signal_name}_fft_power_high': power_high\n",
    "        }\n",
    "        \n",
    "        new_features = {}\n",
    "        valid_len = len(spectral_energy)\n",
    "        \n",
    "        for name, val_array in feats_dict.items():\n",
    "            arr = np.full(N, np.nan)\n",
    "            # Ensure we don't overflow if calculation is slightly off\n",
    "            end_idx = min(N, pad_head + valid_len)\n",
    "            arr[pad_head : end_idx] = val_array[:end_idx-pad_head]\n",
    "            new_features[name] = arr\n",
    "            \n",
    "        X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose: print(f\"FFT feature extraction failed: {e}\")\n",
    "    \n",
    "    return X\n",
    "def add_advanced_kinematics(X, center_x, center_y, fps):\n",
    "    \"\"\"Add acceleration, jerk, and angular velocity features.\"\"\"\n",
    "    # Velocity\n",
    "    vel_x = center_x.diff() * fps  # cm/s\n",
    "    vel_y = center_y.diff() * fps\n",
    "    speed = safe_sqrt(vel_x**2 + vel_y**2)\n",
    "    \n",
    "    # Acceleration (cm/s^2)\n",
    "    acc_x = vel_x.diff() * fps\n",
    "    acc_y = vel_y.diff() * fps\n",
    "    acc_mag = safe_sqrt(acc_x**2 + acc_y**2)\n",
    "    \n",
    "    # Jerk (cm/s^3)\n",
    "    jerk_x = acc_x.diff() * fps\n",
    "    jerk_y = acc_y.diff() * fps\n",
    "    jerk_mag = safe_sqrt(jerk_x**2 + jerk_y**2)\n",
    "    \n",
    "    # Direction angle\n",
    "    direction = np.arctan2(vel_y, vel_x)\n",
    "    \n",
    "    # Angular velocity (rad/s)\n",
    "    angular_vel = direction.diff() * fps\n",
    "    # Unwrap to handle -pi to pi discontinuities\n",
    "    angular_vel = np.where(angular_vel > np.pi, angular_vel - 2*np.pi, angular_vel)\n",
    "    angular_vel = np.where(angular_vel < -np.pi, angular_vel + 2*np.pi, angular_vel)\n",
    "    \n",
    "    # Add raw features\n",
    "    new_features = {}\n",
    "    new_features['speed'] = speed\n",
    "    new_features['acc_mag'] = acc_mag\n",
    "    new_features['jerk_mag'] = jerk_mag\n",
    "    new_features['ang_vel'] = angular_vel\n",
    "    \n",
    "    # Add rolling statistics\n",
    "    for w in [15, 30]:\n",
    "        ws = _scale(w, fps)\n",
    "        roll = dict(window=ws, min_periods=max(1, ws // 4))\n",
    "        new_features[f'acc_mean_{w}'] = acc_mag.rolling(**roll).mean()\n",
    "        new_features[f'acc_std_{w}'] = acc_mag.rolling(**roll).var().clip(lower=0).pow(0.5)\n",
    "        new_features[f'jerk_mean_{w}'] = jerk_mag.rolling(**roll).mean()\n",
    "        new_features[f'ang_vel_std_{w}'] = pd.Series(angular_vel).rolling(**roll).var().clip(lower=0).pow(0.5)\n",
    "    \n",
    "    if new_features:\n",
    "        X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_body_angle_features(X, nose_x, nose_y, body_x, body_y, tail_x, tail_y, fps):\n",
    "    \"\"\"Add head/body angle features relative to arena.\"\"\"\n",
    "    # Head direction (nose relative to body center)\n",
    "    head_dir_x = nose_x - body_x\n",
    "    head_dir_y = nose_y - body_y\n",
    "    head_angle = np.arctan2(head_dir_y, head_dir_x)  # angle relative to arena\n",
    "    \n",
    "    # Body direction (body center relative to tail)\n",
    "    body_dir_x = body_x - tail_x\n",
    "    body_dir_y = body_y - tail_y\n",
    "    body_angle = np.arctan2(body_dir_y, body_dir_x)\n",
    "    \n",
    "    # Angular velocities\n",
    "    head_ang_vel = head_angle.diff() * fps\n",
    "    body_ang_vel = body_angle.diff() * fps\n",
    "    \n",
    "    # Unwrap discontinuities\n",
    "    head_ang_vel = np.where(head_ang_vel > np.pi, head_ang_vel - 2*np.pi, head_ang_vel)\n",
    "    head_ang_vel = np.where(head_ang_vel < -np.pi, head_ang_vel + 2*np.pi, head_ang_vel)\n",
    "    body_ang_vel = np.where(body_ang_vel > np.pi, body_ang_vel - 2*np.pi, body_ang_vel)\n",
    "    body_ang_vel = np.where(body_ang_vel < -np.pi, body_ang_vel + 2*np.pi, body_ang_vel)\n",
    "    \n",
    "    new_features = {}\n",
    "    new_features['head_angle'] = head_angle\n",
    "    new_features['body_angle'] = body_angle\n",
    "    new_features['head_ang_vel'] = head_ang_vel\n",
    "    new_features['body_ang_vel'] = body_ang_vel\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for w in [15, 30]:\n",
    "        ws = _scale(w, fps)\n",
    "        new_features[f'head_ang_std_{w}'] = pd.Series(head_angle).rolling(ws, min_periods=max(1, ws//4)).var().clip(lower=0).pow(0.5)\n",
    "        new_features[f'head_ang_vel_mean_{w}'] = pd.Series(head_ang_vel).rolling(ws, min_periods=max(1, ws//4)).mean()\n",
    "    \n",
    "    if new_features:\n",
    "        X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_enhanced_social_features(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    \"\"\"Add approach angle and facing target features.\"\"\"\n",
    "    if not all(p in avail_A for p in ['nose', 'tail_base', 'body_center']):\n",
    "        return X\n",
    "    if not all(p in avail_B for p in ['nose', 'tail_base', 'body_center']):\n",
    "        return X\n",
    "    \n",
    "    # Relative position\n",
    "    rel_x = mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x']\n",
    "    rel_y = mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y']\n",
    "    \n",
    "    # A's heading direction\n",
    "    A_head_x = mouse_pair['A']['nose']['x'] - mouse_pair['A']['tail_base']['x']\n",
    "    A_head_y = mouse_pair['A']['nose']['y'] - mouse_pair['A']['tail_base']['y']\n",
    "    A_heading = np.arctan2(A_head_y, A_head_x)\n",
    "    \n",
    "    # Angle from A to B\n",
    "    angle_to_B = np.arctan2(rel_y, rel_x)\n",
    "    \n",
    "    # Approach angle: difference between heading and direction to other mouse\n",
    "    approach_angle = A_heading - angle_to_B\n",
    "    # Normalize to [-pi, pi]\n",
    "    approach_angle = approach_angle.replace([np.inf, -np.inf], np.nan)\n",
    "    approach_angle = np.arctan2(np.sin(approach_angle), np.cos(approach_angle))\n",
    "    \n",
    "    new_features = {}\n",
    "    new_features['approach_angle'] = approach_angle\n",
    "    \n",
    "    # Binary facing indicator (facing if angle < 45 degrees)\n",
    "    new_features['facing_target'] = (np.abs(approach_angle.fillna(np.pi)) < np.pi/4).astype(float)    \n",
    "    # Continuous facing feature (Cosine)\n",
    "    new_features['facing_cosine'] = np.cos(approach_angle)\n",
    "    \n",
    "    # Approach speed (Projected Velocity of A towards B)\n",
    "    # 1. Vector from A to B\n",
    "    vec_AB_x = mouse_pair['B']['body_center']['x'] - mouse_pair['A']['body_center']['x']\n",
    "    vec_AB_y = mouse_pair['B']['body_center']['y'] - mouse_pair['A']['body_center']['y']\n",
    "    dist_AB = safe_sqrt(vec_AB_x**2 + vec_AB_y**2) + 1e-6\n",
    "\n",
    "    # 2. Velocity of A\n",
    "    vel_A_x = mouse_pair['A']['body_center']['x'].diff().fillna(0)\n",
    "    vel_A_y = mouse_pair['A']['body_center']['y'].diff().fillna(0)\n",
    "\n",
    "    # 3. Projected velocity\n",
    "    new_features['approach_speed'] = (vel_A_x * vec_AB_x + vel_A_y * vec_AB_y) / dist_AB * fps\n",
    "\n",
    "    \n",
    "    # Rolling statistics\n",
    "    ws = _scale(30, fps)\n",
    "    new_features['facing_pct_30'] = new_features['facing_target'].rolling(ws, min_periods=max(1, ws//6)).mean()\n",
    "    \n",
    "    if new_features:\n",
    "        X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_egocentric_features(X, mouse_df, fps):\n",
    "    \"\"\"\n",
    "    Biến đổi tọa độ sang hệ quy chiếu lấy chuột làm tâm (Egocentric).\n",
    "    Chuẩn hóa sao cho: Body Center tại (0,0), Mũi hướng về phía dương trục X.\n",
    "    \"\"\"\n",
    "    # 1. Xác định trục cơ thể (Spine Vector)\n",
    "    if not all(p in mouse_df.columns.get_level_values(0) for p in ['nose', 'body_center']):\n",
    "        return X\n",
    "\n",
    "    # Vector từ tâm đến mũi\n",
    "    dx = mouse_df['nose']['x'] - mouse_df['body_center']['x']\n",
    "    dy = mouse_df['nose']['y'] - mouse_df['body_center']['y']\n",
    "    \n",
    "    # Góc quay của chuột so với trục hoành của camera\n",
    "    angle = np.arctan2(dy, dx)\n",
    "    cos_a = np.cos(-angle)\n",
    "    sin_a = np.sin(-angle)\n",
    "\n",
    "    new_feats = {}\n",
    "    \n",
    "    # 2. Xoay tọa độ các bộ phận quan trọng\n",
    "    # Chỉ quan tâm các bộ phận chính để giảm chiều dữ liệu\n",
    "    key_parts = ['ear_left', 'ear_right', 'tail_base', 'tail_tip']\n",
    "    available_parts = mouse_df.columns.get_level_values(0)\n",
    "\n",
    "    for part in key_parts:\n",
    "        if part in available_parts:\n",
    "            # Tọa độ tương đối so với tâm\n",
    "            rx = mouse_df[part]['x'] - mouse_df['body_center']['x']\n",
    "            ry = mouse_df[part]['y'] - mouse_df['body_center']['y']\n",
    "            \n",
    "            # Phép quay ma trận 2D\n",
    "            # x_new = x*cos - y*sin\n",
    "            # y_new = x*sin + y*cos\n",
    "            x_rot = rx * cos_a - ry * sin_a\n",
    "            y_rot = rx * sin_a + ry * cos_a\n",
    "            \n",
    "            new_feats[f'ego_x_{part}'] = x_rot\n",
    "            new_feats[f'ego_y_{part}'] = y_rot\n",
    "\n",
    "    if new_feats:\n",
    "        X = pd.concat([X, pd.DataFrame(new_feats, index=X.index)], axis=1)\n",
    "        \n",
    "    return X\n",
    "\n",
    "def add_grooming_features(X, mouse_df, fps):\n",
    "    \"\"\"\n",
    "    Phát hiện hành vi chải chuốt: Thân đứng yên nhưng đầu di chuyển/rung lắc.\n",
    "    \"\"\"\n",
    "    if not all(p in mouse_df.columns.get_level_values(0) for p in ['nose', 'body_center']):\n",
    "        return X\n",
    "\n",
    "    # Tốc độ mũi\n",
    "    nose_speed = np.sqrt(mouse_df['nose']['x'].diff()**2 + mouse_df['nose']['y'].diff()**2) * fps\n",
    "    # Tốc độ thân\n",
    "    body_speed = np.sqrt(mouse_df['body_center']['x'].diff()**2 + mouse_df['body_center']['y'].diff()**2) * fps\n",
    "\n",
    "    # Tỷ lệ tách biệt (Decoupling Ratio)\n",
    "    # Thêm 1e-3 để tránh chia cho 0\n",
    "    decouple = nose_speed / (body_speed + 1e-3)\n",
    "    \n",
    "    # Làm mượt (Smoothing) vì hành vi này thường kéo dài ít nhất 0.5s\n",
    "    w = int(0.5 * fps)\n",
    "    \n",
    "    new_feats = {}\n",
    "    new_feats['head_body_ratio'] = decouple.rolling(w).median()\n",
    "    new_feats['body_immobile'] = (body_speed < 2.0).astype(float) # Thân di chuyển dưới 2cm/s\n",
    "    new_feats['nose_active'] = (nose_speed > 5.0).astype(float)   # Mũi di chuyển trên 5cm/s\n",
    "    \n",
    "    # Kết hợp logic: Grooming = Thân tĩnh AND Mũi động\n",
    "    new_feats['grooming_score'] = new_feats['body_immobile'] * new_feats['nose_active']\n",
    "\n",
    "    X = pd.concat([X, pd.DataFrame(new_feats, index=X.index)], axis=1)\n",
    "    return X\n",
    "\n",
    "def add_temporal_asymmetry(X, center_x, center_y, fps):\n",
    "    \"\"\"\n",
    "    So sánh vận tốc tương lai và quá khứ để phát hiện chuyển đổi trạng thái (Attack onset/offset).\n",
    "    \"\"\"\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * fps\n",
    "    \n",
    "    # Cửa sổ 1 giây\n",
    "    w = int(1.0 * fps)\n",
    "    \n",
    "    # Vận tốc trung bình Quá khứ (shift dương)\n",
    "    # min_periods=1 để tránh NaN ở đầu video\n",
    "    past_mean = speed.rolling(window=w, min_periods=1).mean()\n",
    "    \n",
    "    # Vận tốc trung bình Tương lai (shift ngược bằng cách đảo ngược chuỗi)\n",
    "    # Hoặc dùng: speed.shift(-w).rolling(w).mean() nhưng cách dưới chính xác hơn cho biên\n",
    "    future_mean = speed.iloc[::-1].rolling(window=w, min_periods=1).mean().iloc[::-1]\n",
    "    \n",
    "    new_feats = {}\n",
    "    # Delta V: Dương -> Đang tăng tốc (Attack start), Âm -> Đang giảm tốc (Stop)\n",
    "    new_feats['accel_trend_1s'] = future_mean - past_mean\n",
    "    \n",
    "    # Ratio: Thay đổi gấp bao nhiêu lần\n",
    "    new_feats['accel_ratio_1s'] = future_mean / (past_mean + 1e-3)\n",
    "\n",
    "    X = pd.concat([X, pd.DataFrame(new_feats, index=X.index)], axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:54:56.926953Z",
     "iopub.status.busy": "2025-12-03T17:54:56.926726Z",
     "iopub.status.idle": "2025-12-03T17:54:56.966286Z",
     "shell.execute_reply": "2025-12-03T17:54:56.96559Z"
    },
    "papermill": {
     "duration": 0.044204,
     "end_time": "2025-12-03T17:54:56.967374",
     "exception": false,
     "start_time": "2025-12-03T17:54:56.92317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_body_parts =  ['headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n",
    "                    'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n",
    "                    'spine_1', 'spine_2',\n",
    "                    'tail_middle_1', 'tail_middle_2', 'tail_midpoint']\n",
    "\n",
    "def generate_mouse_data(dataset, traintest, traintest_directory=None, generate_single=True, generate_pair=True):\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"{CFG.BASE_PATH}/{traintest}_tracking\"\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        lab_id = row.lab_id\n",
    "        if lab_id.startswith('MABe22') and traintest == 'train': continue #MABe22 kh co label\n",
    "        video_id = row.video_id\n",
    "\n",
    "        if type(row.behaviors_labeled) != str: # Nếu không có nhãn đánh dấu -> skip\n",
    "            print('No labeled behaviors:', lab_id, video_id, type(row.behaviors_labeled), row.behaviors_labeled)\n",
    "            continue\n",
    "\n",
    "        path = f'{traintest_directory}/{lab_id}/{video_id}.parquet'\n",
    "        vid = pd.read_parquet(path)\n",
    "\n",
    "        if len(np.unique(vid.bodypart)) > 5:\n",
    "            vid = vid.query(\"~ bodypart.isin(@drop_body_parts)\")\n",
    "\n",
    "        pvid = vid.pivot(columns=['mouse_id', 'bodypart'], index='video_frame', values=['x', 'y'])\n",
    "\n",
    "        if (pvid.isna().any().any()):\n",
    "            if verbose and traintest == 'test': print('video with missing values', video_id, traintest, len(vid), 'frames')\n",
    "        else:\n",
    "            if verbose and traintest == 'test': print('video with all values', video_id, traintest, len(vid), 'frames')\n",
    "        del vid\n",
    "\n",
    "        pvid = pvid.reorder_levels([1,2,0], axis=1).T.sort_index().T\n",
    "        \n",
    "        # 1. Đổi đơn vị: pixels -> cm\n",
    "        pvid /= row.pix_per_cm_approx\n",
    "\n",
    "        vid_behaviors = json.loads(row.behaviors_labeled)\n",
    "        vid_behaviors = sorted(list({b.replace(\"''\",\"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n",
    "    \n",
    "        if traintest == 'train':\n",
    "            try:\n",
    "                annot = pd.read_parquet(path.replace('train_tracking', 'train_annotation'))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        if generate_single:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target == 'self'\")\n",
    "\n",
    "            for mouse_id_str in np.unique(vid_behaviors_subset.agent):\n",
    "                try:\n",
    "                    mouse_id = int(mouse_id_str[-1])\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"agent == @mouse_id_str\").action)\n",
    "\n",
    "                    single_mouse = pvid.loc[:, mouse_id]\n",
    "                    assert len(single_mouse) == len(pvid)\n",
    "                    single_mouse_meta = pd.DataFrame({\n",
    "                        'video_id': video_id,\n",
    "                        'agent_id': mouse_id_str,\n",
    "                        'target_id': 'self',\n",
    "                        'video_frame': single_mouse.index\n",
    "                    })\n",
    "\n",
    "                    if traintest == 'train':\n",
    "                        single_mouse_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=single_mouse.index) \n",
    "                        annot_subset = annot.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            single_mouse_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n",
    "                        yield 'single', single_mouse, single_mouse_meta, single_mouse_label\n",
    "                    else:\n",
    "                        if verbose: print('- test single', video_id, mouse_id)\n",
    "                        yield 'single', single_mouse, single_mouse_meta, vid_agent_actions\n",
    "\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        if generate_pair:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target != 'self'\")\n",
    "            if len(vid_behaviors_subset) > 0:\n",
    "                for agent, target in itertools.permutations(np.unique(pvid.columns.get_level_values('mouse_id')), 2):\n",
    "                    agent_str = f'mouse{agent}'\n",
    "                    target_str = f'mouse{target}'\n",
    "\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"(agent == @agent_str) & (target == @target_str)\").action)\n",
    "                    mouse_pair = pd.concat([pvid[agent], pvid[target]], axis=1, keys=['A', 'B'])\n",
    "                    assert len(mouse_pair) == len(pvid)\n",
    "                   \n",
    "                    mouse_pair_meta = pd.DataFrame({\n",
    "                        'video_id': video_id,\n",
    "                        'agent_id': agent_str,\n",
    "                        'target_id': target_str,\n",
    "                        'video_frame': mouse_pair.index\n",
    "                    })\n",
    "                    \n",
    "                    if traintest == 'train':\n",
    "                        mouse_pair_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=mouse_pair.index)\n",
    "                        annot_subset = annot.query(\"(agent_id == @agent) & (target_id == @target)\")\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            mouse_pair_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n",
    "                        yield 'pair', mouse_pair, mouse_pair_meta, mouse_pair_label\n",
    "                    else:\n",
    "                        if verbose: print('- test pair', video_id, agent, target)\n",
    "                        yield 'pair', mouse_pair, mouse_pair_meta, vid_agent_actions\n",
    "def transform_single(single_mouse, body_parts_tracked, fps):\n",
    "    \"\"\"Transform from cartesian coordinates to distance representation.\n",
    "\n",
    "    Parameters:\n",
    "    single_mouse: dataframe with coordinates of the body parts of one mouse\n",
    "                  shape (n_samples, n_body_parts * 2)\n",
    "                  two-level MultiIndex on columns\n",
    "    body_parts_tracked: list of body parts\n",
    "    \"\"\"\n",
    "    available_body_parts = single_mouse.columns.get_level_values(0)\n",
    "    \n",
    "    # Smooth coordinates to reduce noise\n",
    "    single_mouse = smooth_coordinates(single_mouse, sigma=1.5)\n",
    "    \n",
    "    # X là toàn bộ khoảng cách giữa các bộ phận của con chuột\n",
    "    X = pd.DataFrame({\n",
    "            f\"{part1}+{part2}\": np.square(single_mouse[part1] - single_mouse[part2]).sum(axis=1, skipna=False)\n",
    "            for part1, part2 in itertools.combinations(body_parts_tracked, 2) if part1 in available_body_parts and part2 in available_body_parts\n",
    "        })\n",
    "    X = X.reindex(columns=[f\"{part1}+{part2}\" for part1, part2 in itertools.combinations(body_parts_tracked, 2)], copy=False)\n",
    "\n",
    "    if all(p in single_mouse.columns for p in ['ear_left', 'ear_right', 'tail_base']):\n",
    "        # lag ~ 10 frame trong 30fps\n",
    "        lag = _scale(10, fps) \n",
    "        shifted = single_mouse[['ear_left', 'ear_right', 'tail_base']].shift(lag) \n",
    "        X = pd.concat([\n",
    "            X, \n",
    "            pd.DataFrame({\n",
    "                'speed_left': np.square(single_mouse['ear_left'] - shifted['ear_left']).sum(axis=1, skipna=False),\n",
    "                'speed_right': np.square(single_mouse['ear_right'] - shifted['ear_right']).sum(axis=1, skipna=False),\n",
    "                'speed_left2': np.square(single_mouse['ear_left'] - shifted['tail_base']).sum(axis=1, skipna=False),\n",
    "                'speed_right2': np.square(single_mouse['ear_right'] - shifted['tail_base']).sum(axis=1, skipna=False),\n",
    "            })\n",
    "        ], axis=1)\n",
    "\n",
    "    new_features = {}\n",
    "    # Elongation: độ kéo dài của cơ thể\n",
    "    if 'nose+tail_base' in X.columns and \"ear_left+ear_right\" in X.columns:\n",
    "        new_features['elong'] = safe_sqrt(X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6))\n",
    "\n",
    "    # Góc từ mũi -> thân -> đuôi\n",
    "    if all(p in available_body_parts for p in ['nose', 'body_center', 'tail_base']):\n",
    "        v1 = single_mouse['nose']-single_mouse['body_center']\n",
    "        v2 = single_mouse['tail_base'] - single_mouse['body_center']\n",
    "        new_features['body_ang'] = (v1['x'] * v2['x'] + v1['y'] * v2['y']) / (safe_sqrt(v1['x']**2 + v1['y']**2) * safe_sqrt(v2['x']**2 + v2['y']**2))\n",
    "\n",
    "    # Khoảng cách giữa mũi và đuôi\n",
    "    if all(p in available_body_parts for p in ['nose', 'tail_base']):\n",
    "        nt_dist = safe_sqrt((single_mouse['nose']['x'] - single_mouse['tail_base']['x'])**2 +\n",
    "                          (single_mouse['nose']['y'] - single_mouse['tail_base']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            new_features[f'nt_lg{lag}'] = nt_dist.shift(l) # khoảng cách giữa nose-tail trong quá khứ\n",
    "            new_features[f'nt_df{lag}'] = nt_dist - nt_dist.shift(l) # Độ thay đổi so với hiện tại\n",
    "\n",
    "    # Rolling statistic dựa trên body center\n",
    "    if 'body_center' in available_body_parts:\n",
    "        center_x = single_mouse['body_center']['x']\n",
    "        center_y = single_mouse['body_center']['y']\n",
    "\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            w_scale = _scale(w, fps)\n",
    "            roll = dict(window=w_scale, min_periods=1, center=True)\n",
    "            new_features[f'cx_mean_{w}'] = center_x.rolling(**roll).mean()\n",
    "            new_features[f'cy_mean_{w}'] = center_y.rolling(**roll).mean()\n",
    "            new_features[f'cx_std_{w}'] = center_x.rolling(**roll).var().clip(lower=0).pow(0.5)\n",
    "            new_features[f'cy_std_{w}'] = center_y.rolling(**roll).var().clip(lower=0).pow(0.5)\n",
    "            new_features[f'cx_range_{w}'] = center_x.rolling(**roll).max() - center_x.rolling(**roll).min()\n",
    "            new_features[f'cy_range_{w}'] = center_y.rolling(**roll).max() - center_y.rolling(**roll).min()\n",
    "            new_features[f'variablitiy_{w}'] = safe_sqrt(center_x.diff().rolling(w_scale, min_periods=1).var().clip(lower=0) + \n",
    "                                             center_y.diff().rolling(w_scale, min_periods=1).var().clip(lower=0))\n",
    "            new_features[f'displacement_{w}'] = safe_sqrt(center_x.diff().rolling(w_scale, min_periods=1).sum()**2 + \n",
    "                                              center_y.diff().rolling(w_scale, min_periods=1).sum()**2)\n",
    "            \n",
    "    if new_features:\n",
    "        X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "\n",
    "    # Call helper functions (they now use pd.concat internally)\n",
    "    if 'body_center' in available_body_parts:\n",
    "        X = add_curvature_features(X, center_x, center_y, fps)\n",
    "        X = add_multiscale_features(X, center_x, center_y, fps)\n",
    "        X = add_state_features(X, center_x, center_y, fps)\n",
    "        X = add_longrange_features(X, center_x, center_y, fps)\n",
    "        \n",
    "        # New features: Advanced kinematics\n",
    "        X = add_advanced_kinematics(X, center_x, center_y, fps)\n",
    "        \n",
    "        # New features: FFT on speed signal\n",
    "        speed_signal = safe_sqrt(center_x.diff()**2 + center_y.diff()**2)\n",
    "        X = add_fft_features(X, speed_signal, 'speed', fps, window_size=120)\n",
    "        \n",
    "        # New features: Body angles (if body parts available)\n",
    "        if all(p in available_body_parts for p in ['nose', 'body_center', 'tail_base']):\n",
    "            X = add_body_angle_features(X, \n",
    "                                       single_mouse['nose']['x'], single_mouse['nose']['y'],\n",
    "                                       center_x, center_y,\n",
    "                                       single_mouse['tail_base']['x'], single_mouse['tail_base']['y'],\n",
    "                                       fps)\n",
    "\n",
    "    if all(p in available_body_parts for p in ['ear_left', 'ear_right']):\n",
    "        ear_dist = safe_sqrt((single_mouse['ear_left']['x'] - single_mouse['ear_right']['x'])**2 + \n",
    "                           (single_mouse['ear_left']['y'] - single_mouse['ear_right']['y'])**2)\n",
    "        \n",
    "        new_features_ear = {}\n",
    "        for offset in [-30, -20, -10, 10, 20, 30]:\n",
    "            o = _scale_signed(offset, fps)\n",
    "            new_features_ear[f'ear_dist_o{offset}'] = ear_dist.shift(-o)\n",
    "        \n",
    "        w = _scale(30, fps)\n",
    "        new_features_ear['ear_consistency'] = ear_dist.rolling(w, min_periods=1, center=True).var().clip(lower=0).pow(0.5) / (ear_dist.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "        \n",
    "        if new_features_ear:\n",
    "            X = pd.concat([X, pd.DataFrame(new_features_ear, index=X.index)], axis=1)\n",
    "\n",
    "# --- ĐOẠN CODE THÊM MỚI ---\n",
    "    # 1. Thêm Egocentric (Quan trọng nhất)\n",
    "    X = add_egocentric_features(X, single_mouse, fps)\n",
    "    \n",
    "    # 2. Thêm Grooming Features (Cải thiện lớp 'Grooming' và 'Other')\n",
    "    X = add_grooming_features(X, single_mouse, fps)\n",
    "    \n",
    "    # 3. Thêm Temporal Asymmetry (Cải thiện lớp 'Attack' và 'Chase')\n",
    "    if 'body_center' in available_body_parts:\n",
    "        cx = single_mouse['body_center']['x']\n",
    "        cy = single_mouse['body_center']['y']\n",
    "        X = add_temporal_asymmetry(X, cx, cy, fps)\n",
    "    # --------------------------\n",
    "    return X.astype(np.float32, copy=False)\n",
    "\n",
    "def transform_pair(mouse_pair, body_parts_tracked, fps):\n",
    "    \"\"\"Transform from cartesian coordinates to distance representation.\n",
    "\n",
    "    Parameters:\n",
    "    mouse_pair: dataframe with coordinates of the body parts of two mice\n",
    "                  shape (n_samples, 2 * n_body_parts * 2)\n",
    "                  three-level MultiIndex on columns\n",
    "    body_parts_tracked: list of body parts\n",
    "    \"\"\"\n",
    "    # drop_body_parts =  ['ear_left', 'ear_right',\n",
    "    #                     'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n",
    "    #                     'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n",
    "    #                     'tail_midpoint']\n",
    "    # if len(body_parts_tracked) > 5:\n",
    "    #     body_parts_tracked = [b for b in body_parts_tracked if b not in drop_body_parts]\n",
    "    available_body_parts_A = mouse_pair['A'].columns.get_level_values(0)\n",
    "    available_body_parts_B = mouse_pair['B'].columns.get_level_values(0)\n",
    "    \n",
    "    # Smooth coordinates to reduce noise\n",
    "    mouse_pair['A'] = smooth_coordinates(mouse_pair['A'], sigma=1.5)\n",
    "    mouse_pair['B'] = smooth_coordinates(mouse_pair['B'], sigma=1.5)\n",
    "    \n",
    "    ETHOLOGICAL_PAIRS = [\n",
    "        ('nose', 'nose'),\n",
    "        ('nose', 'tail_base'),\n",
    "        ('tail_base', 'nose'),\n",
    "        ('nose', 'body_center'),\n",
    "        ('body_center', 'nose'),\n",
    "        ('body_center', 'body_center'),\n",
    "        ('tail_base', 'tail_base')\n",
    "    ]\n",
    "    \n",
    "    X = pd.DataFrame({\n",
    "            f\"12+{part1}+{part2}\": np.square(mouse_pair['A'][part1] - mouse_pair['B'][part2]).sum(axis=1, skipna=False)\n",
    "            for part1, part2 in ETHOLOGICAL_PAIRS if part1 in available_body_parts_A and part2 in available_body_parts_B\n",
    "        })\n",
    "    X = X.reindex(columns=[f\"12+{part1}+{part2}\" for part1, part2 in ETHOLOGICAL_PAIRS], copy=False)\n",
    "\n",
    "    if ('A', 'ear_left') in mouse_pair.columns and ('B', 'ear_left') in mouse_pair.columns:\n",
    "        lag = _scale(10, fps)\n",
    "        shifted_A = mouse_pair['A']['ear_left'].shift(lag)\n",
    "        shifted_B = mouse_pair['B']['ear_left'].shift(lag)\n",
    "        X = pd.concat([\n",
    "            X,\n",
    "            pd.DataFrame({\n",
    "                'speed_left_A': np.square(mouse_pair['A']['ear_left'] - shifted_A).sum(axis=1, skipna=False),\n",
    "                'speed_left_AB': np.square(mouse_pair['A']['ear_left'] - shifted_B).sum(axis=1, skipna=False),\n",
    "                'speed_left_B': np.square(mouse_pair['B']['ear_left'] - shifted_B).sum(axis=1, skipna=False),\n",
    "            })\n",
    "        ], axis=1)\n",
    "\n",
    "    new_features = {}\n",
    "    # góc giữa 2 con chuột\n",
    "    if all(p in available_body_parts_A for p in ['nose', 'tail_base']) and all(p in available_body_parts_B for p in ['nose', 'tail_base']):\n",
    "        dir_A = mouse_pair['A']['nose'] - mouse_pair['A']['tail_base']\n",
    "        dir_B = mouse_pair['B']['nose'] - mouse_pair['B']['tail_base']\n",
    "        new_features['rel_ori'] = (dir_A['x'] * dir_B['x'] + dir_A['y'] * dir_B['y']) / (\n",
    "            safe_sqrt(dir_A['x']**2 + dir_A['y']**2) * safe_sqrt(dir_B['x']**2 + dir_B['y']**2) + 1e-6)\n",
    "        \n",
    "    # Khoảng cách giữa 2 mũi của 2 con chuột, % chúng gần\n",
    "    if all(p in available_body_parts_A for p in ['nose']) and all(p in available_body_parts_B for p in ['nose']):\n",
    "        nose_nose = safe_sqrt((mouse_pair['A']['nose']['x'] - mouse_pair['B']['nose']['x'])**2 +\n",
    "                     (mouse_pair['A']['nose']['y'] - mouse_pair['B']['nose']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            lag_ = _scale(lag, fps)\n",
    "            new_features[f'nose-nose_lag_{lag}'] = nose_nose.shift(lag_) # Khoảng cách của mũi chúng theo lag\n",
    "            new_features[f'nose-nose_change_{lag}'] = nose_nose - nose_nose.shift(lag_) # Sự thay đổi khoảng cách của chúng\n",
    "            is_close = nose_nose.fillna(np.inf).lt(10).astype(float)\n",
    "            new_features[f'close_percentage_{lag}'] = is_close.rolling(lag_, min_periods=1).mean() # Trong _lag frame thì có bao nhiêu % là mũi của chúng gần\n",
    "    \n",
    "    # if 'body_center' in available_body_parts_A and 'body_center' in available_body_parts_B:\n",
    "    #     cd = safe_sqrt((mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x'])**2 +\n",
    "    #                  (mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y'])**2)\n",
    "    #     cd = cd.fillna(np.inf)\n",
    "    #     X['very_close'] = (cd < 5.0).astype(float)\n",
    "    #     X['close']   = ((cd >= 5.0) & (cd < 15.0)).astype(float)\n",
    "    #     X['med']   = ((cd >= 15.0) & (cd < 30.0)).astype(float)\n",
    "    #     X['far']   = (cd >= 30.0).astype(float)\n",
    "\n",
    "    # Thống kê dựa trên khoảng cách của body center\n",
    "    if 'body_center' in available_body_parts_A and 'body_center' in available_body_parts_B:\n",
    "        center_d = np.square(mouse_pair['A']['body_center'] - mouse_pair['B']['body_center']).sum(axis=1, skipna=False)\n",
    "        Avx = mouse_pair['A']['body_center']['x'].diff()\n",
    "        Avy = mouse_pair['A']['body_center']['y'].diff()\n",
    "        Bvx = mouse_pair['B']['body_center']['x'].diff()\n",
    "        Bvy = mouse_pair['B']['body_center']['y'].diff()\n",
    "        coord = Avx * Bvx + Avy * Bvy\n",
    "        val = (Avx * Bvx + Avy * Bvy) / (safe_sqrt(Avx**2 + Avy**2) * safe_sqrt(Bvx**2 + Bvy**2) + 1e-6)\n",
    "\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            w_scale = _scale(w, fps)\n",
    "            roll = dict(window=w_scale, min_periods=1, center=True)\n",
    "            new_features[f'd_mean_{w}'] = center_d.rolling(**roll).mean()\n",
    "            new_features[f'd_std_{w}'] = center_d.rolling(**roll).var().clip(lower=0).pow(0.5)\n",
    "            new_features[f'd_max_{w}'] = center_d.rolling(**roll).max()\n",
    "            new_features[f'd_min_{w}'] = center_d.rolling(**roll).min()\n",
    "            d_var = center_d.rolling(**roll).var()\n",
    "            new_features[f'interaction_{w}'] = 1 / (1 + d_var) # neu var thap -> khoang cach on dinh -> int cao\n",
    "            new_features[f'co_m{w}'] = coord.rolling(**roll).mean()\n",
    "            new_features[f'co_s{w}'] = coord.rolling(**roll).var().clip(lower=0).pow(0.5)\n",
    "        \n",
    "        for off in [-30, -20, -10, 0, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            new_features[f'va_{off}'] = val.shift(-o)\n",
    "        ws = _scale(30, fps)\n",
    "        new_features['int_con'] = center_d.rolling(ws, min_periods=1, center=True).var().clip(lower=0).pow(0.5) / \\\n",
    "                       (center_d.rolling(ws, min_periods=1, center=True).mean() + 1e-6)\n",
    "        \n",
    "    if new_features:\n",
    "        X = pd.concat([X, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "\n",
    "    if 'body_center' in available_body_parts_A and 'body_center' in available_body_parts_B:\n",
    "        X = add_interaction_features(X, mouse_pair, available_body_parts_A, available_body_parts_B, fps)\n",
    "        \n",
    "        # New features: Enhanced social interaction features\n",
    "        X = add_enhanced_social_features(X, mouse_pair, available_body_parts_A, available_body_parts_B, fps)\n",
    "        \n",
    "        # New features: FFT on inter-mouse distance\n",
    "        if 'body_center' in available_body_parts_A and 'body_center' in available_body_parts_B:\n",
    "            inter_dist = safe_sqrt((mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x'])**2 +\n",
    "                                (mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y'])**2)\n",
    "            X = add_fft_features(X, inter_dist, 'dist', fps, window_size=120)\n",
    "\n",
    "    # Replace inf values with 0\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    # Replace inf values with 0\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    return X.astype(np.float32, copy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSampler(ClassifierMixin, BaseEstimator):\n",
    "    def __init__(self, estimator, neg_pos_ratio=10.0): \n",
    "        self.estimator = estimator\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_arr = np.array(X, copy=False)\n",
    "        y_arr = np.array(y, copy=False)\n",
    "        \n",
    "        pos_indices = np.where(y_arr == 1)[0]\n",
    "        neg_indices = np.where(y_arr == 0)[0]\n",
    "        \n",
    "        if len(pos_indices) == 0:\n",
    "            self.estimator.fit(X_arr[::10], y_arr[::10])\n",
    "            self.classes_ = self.estimator.classes_\n",
    "            return self\n",
    "\n",
    "        n_neg_keep = int(len(pos_indices) * self.neg_pos_ratio)\n",
    "        \n",
    "        if len(neg_indices) > n_neg_keep:\n",
    "            kept_neg_indices = np.random.choice(neg_indices, n_neg_keep, replace=False)\n",
    "        else:\n",
    "            kept_neg_indices = neg_indices\n",
    "            \n",
    "        final_indices = np.concatenate([pos_indices, kept_neg_indices])\n",
    "        np.random.shuffle(final_indices)\n",
    "        \n",
    "        self.estimator.fit(X_arr[final_indices], y_arr[final_indices])\n",
    "        self.classes_ = self.estimator.classes_\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(np.array(X))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(np.array(X))\n",
    "\n",
    "\n",
    "def submit(body_parts_tracked_str, switch_tr, X_tr, label, meta):\n",
    "    \"\"\"Produce a submission file for the selected subset of the test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    body_parts_tracked_str: subset of body parts for filtering the test set\n",
    "    switch_tr: 'single' or 'pair'\n",
    "    binary_classifier: classifier with predict_proba\n",
    "    X_tr: training features as 2d array-like of shape (n_samples, n_features)\n",
    "    label: dataframe with binary targets (one column per action, may have missing values), index doesn't matter\n",
    "    meta: dataframe with columns ['video_id', 'agent_id', 'target_id', 'video_frame'], index doesn't matter\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    appends to submission_list\n",
    "    \n",
    "    \"\"\"\n",
    "    # Fit a binary classifier for every action\n",
    "    model_list = [] # will get a model per action\n",
    "    for action in label.columns:\n",
    "        # Filter for samples (video frames) with a defined target (i.e., target is not nan)\n",
    "        action_mask = ~ label[action].isna().values\n",
    "        y_action = label[action][action_mask].values.astype(int)\n",
    "\n",
    "        if not (y_action == 0).all():\n",
    "            # Train XGBoost\n",
    "            model_xgb = DataSampler(clone(CFG.model), neg_pos_ratio=10.0)\n",
    "            model_xgb.fit(X_tr[action_mask], y_action)\n",
    "            \n",
    "            # Train CatBoost\n",
    "            cat_model = CatBoostClassifier(**CAT_PARAMS)\n",
    "            model_cat = DataSampler(cat_model, neg_pos_ratio=10.0)\n",
    "            model_cat.fit(X_tr[action_mask], y_action)\n",
    "\n",
    "            # Feature Importance Logging\n",
    "            try:\n",
    "                # XGBoost\n",
    "                importances_xgb = model_xgb.estimator.feature_importances_\n",
    "                feature_names = X_tr.columns\n",
    "                feature_importance_df_xgb = pd.DataFrame({'Feature': feature_names, 'Importance': importances_xgb})\n",
    "                top_features_xgb = feature_importance_df_xgb.sort_values(by='Importance', ascending=False).head(20)\n",
    "                print(f\"\\nTop 20 Features for Action (XGBoost): {action}\")\n",
    "                for index, row in top_features_xgb.iterrows():\n",
    "                    print(f\"  {row['Feature']}: {row['Importance']:.4f}\")\n",
    "                \n",
    "                # CatBoost\n",
    "                importances_cat = model_cat.estimator.feature_importances_\n",
    "                feature_importance_df_cat = pd.DataFrame({'Feature': feature_names, 'Importance': importances_cat})\n",
    "                top_features_cat = feature_importance_df_cat.sort_values(by='Importance', ascending=False).head(20)\n",
    "                print(f\"\\nTop 20 Features for Action (CatBoost): {action}\")\n",
    "                for index, row in top_features_cat.iterrows():\n",
    "                    print(f\"  {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                if verbose: print(f\"Could not print feature importance for {action}: {e}\")\n",
    "            \n",
    "            model_list.append((action, model_xgb, model_cat))\n",
    "\n",
    "    # Compute test predictions in batches\n",
    "    body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "    if len(body_parts_tracked) > 5:\n",
    "        body_parts_tracked = [b for b in body_parts_tracked if b not in drop_body_parts]\n",
    "    if validate_or_submit == 'submit':\n",
    "        test_subset = test[test.body_parts_tracked == body_parts_tracked_str]\n",
    "        generator = generate_mouse_data(test_subset, 'test',\n",
    "                                        generate_single=(switch_tr == 'single'), \n",
    "                                        generate_pair=(switch_tr == 'pair'))\n",
    "        \n",
    "        fps_lookup = (\n",
    "            test_subset[['video_id', 'frames_per_second']]\n",
    "            .drop_duplicates('video_id')\n",
    "            .set_index('video_id')['frames_per_second']\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "        for switch_te, data_te, meta_te, actions_te in generator:\n",
    "            assert switch_te == switch_tr\n",
    "        try:\n",
    "            # Transform from coordinate representation into distance representation\n",
    "            fps_i = _fps_from_meta(meta_te, fps_lookup)\n",
    "\n",
    "            if switch_te == 'single':\n",
    "                X_te = transform_single(data_te, body_parts_tracked, fps_i) \n",
    "            else:\n",
    "                X_te = transform_pair(data_te, body_parts_tracked, fps_i) \n",
    "            if verbose and len(X_te) == 0: print(\"ERROR: X_te is empty\")\n",
    "            del data_te\n",
    "    \n",
    "            # Compute binary predictions\n",
    "            pred = pd.DataFrame(index=meta_te.video_frame) # will get a column per action\n",
    "            for action, model_xgb, model_cat in model_list:\n",
    "                if action in actions_te:\n",
    "                    p_xgb = model_xgb.predict_proba(X_te)[:, 1]\n",
    "                    p_cat = model_cat.predict_proba(X_te)[:, 1]\n",
    "                    pred[action] = (p_xgb + p_cat) / 2.0\n",
    "            del X_te\n",
    "            # Probability Smoothing\n",
    "            ws = _scale(15, fps_i)\n",
    "            pred = pred.rolling(window=ws, min_periods=1, center=True).mean()\n",
    "\n",
    "            # Compute multiclass predictions\n",
    "            \n",
    "            if pred.shape[1] != 0:\n",
    "                submission_part = predict_multiclass(pred, meta_te)\n",
    "                submission_list.append(submission_part)\n",
    "            else: # this happens if there was no useful training data for the test actions\n",
    "                if verbose: print(f\"  ERROR: no useful training data\")\n",
    "        except KeyError:\n",
    "            if verbose: print(f'  ERROR: KeyError because of missing bodypart ({switch_tr})')\n",
    "            del data_te\n",
    "\n",
    "\n",
    "def predict_multiclass(pred, meta):\n",
    "    \"\"\"Derive multiclass predictions from a set of binary predictions.\n",
    "    \n",
    "    Parameters\n",
    "    pred: dataframe of predicted binary probabilities, shape (n_samples, n_actions), index doesn't matter\n",
    "    meta: dataframe with columns ['video_id', 'agent_id', 'target_id', 'video_frame'], index doesn't matter\n",
    "    \"\"\"\n",
    "    # Default threshold\n",
    "    default_thresh = 0.3\n",
    "    \n",
    "    # Gap fill: khoảng 0.3 giây (~10 frames ở 30fps)\n",
    "    # Min duration: khoảng 0.1 giây (~3-5 frames)\n",
    "    GAP_FILL_SIZE = 10\n",
    "    MIN_DURATION_SIZE = 5 \n",
    "    \n",
    "    # 1. Apply thresholds to create a mask\n",
    "    binary_pred = pd.DataFrame(0, index=pred.index, columns=pred.columns)\n",
    "    \n",
    "    for action in pred.columns:\n",
    "        binary_pred[action] = (pred[action] >= default_thresh).astype(int)\n",
    "\n",
    "    # --- BẮT ĐẦU ĐOẠN POST-PROCESSING MỚI ---\n",
    "    # Áp dụng cho từng cột hành động riêng biệt\n",
    "    for col in binary_pred.columns:\n",
    "        mask = binary_pred[col].values\n",
    "        \n",
    "        # Bước A: Gap Filling (Lấp lỗ hổng)\n",
    "        # Dùng binary_closing: Nối các đoạn 1 bị đứt quãng bởi các số 0 ngắn\n",
    "        structure_gap = np.ones(GAP_FILL_SIZE)\n",
    "        mask_filled = binary_closing(mask, structure=structure_gap).astype(int)\n",
    "        \n",
    "        # Bước B: Min Duration Filtering (Lọc nhiễu ngắn)\n",
    "        # Dùng binary_opening: Xóa các đoạn 1 ngắn hơn kích thước structure\n",
    "        structure_min = np.ones(MIN_DURATION_SIZE)\n",
    "        mask_clean = binary_opening(mask_filled, structure=structure_min).astype(int)\n",
    "        \n",
    "        # Cập nhật lại cột\n",
    "        binary_pred[col] = mask_clean\n",
    "    # --- KẾT THÚC ĐOẠN POST-PROCESSING ---\n",
    "\n",
    "    # 2. Mask the probabilities\n",
    "    # Only keep probabilities for actions that passed the threshold\n",
    "    masked_pred = pred * binary_pred\n",
    "    \n",
    "    # 3. Find the action with the highest probability AMONG those that passed\n",
    "    # Note: If no action passed, masked_pred is all 0s, argmax might return 0 (first index)\n",
    "    # but we will filter those out using has_action_mask.\n",
    "    best_action_indices = np.argmax(masked_pred.values, axis=1)\n",
    "    \n",
    "    final_actions = np.full(len(pred), -1)\n",
    "    \n",
    "    # Only consider rows with at least one action passing threshold\n",
    "    has_action_mask = binary_pred.sum(axis=1) > 0\n",
    "    \n",
    "    final_actions[has_action_mask] = best_action_indices[has_action_mask]\n",
    "    \n",
    "    ama = pd.Series(final_actions, index=meta.video_frame)\n",
    "    # Keep only start and stop frames\n",
    "    changes_mask = (ama != ama.shift(1)).values # nếu ama != ama.shift(1) -> đổi hành động\n",
    "    ama_changes = ama[changes_mask] # Chỉ giữ lại những frame thay đổi hành động\n",
    "    meta_changes = meta[changes_mask]\n",
    "    # mask selects the start frames\n",
    "    mask = ama_changes.values >= 0 # lọc những điểm bắt đầu của các action\n",
    "    mask[-1] = False  \n",
    "    submission_part = pd.DataFrame({\n",
    "        'video_id': meta_changes['video_id'][mask].values,\n",
    "        'agent_id': meta_changes['agent_id'][mask].values,\n",
    "        'target_id': meta_changes['target_id'][mask].values,\n",
    "        'action': pred.columns[ama_changes[mask].values],\n",
    "        'start_frame': ama_changes.index[mask],\n",
    "        'stop_frame': ama_changes.index[1:][mask[:-1]]\n",
    "    })\n",
    "    \n",
    "    # Nếu action kéo đến hết video:\n",
    "    stop_video_id = meta_changes['video_id'][1:][mask[:-1]].values\n",
    "    stop_agent_id = meta_changes['agent_id'][1:][mask[:-1]].values\n",
    "    stop_target_id = meta_changes['target_id'][1:][mask[:-1]].values\n",
    "    for i in range(len(submission_part)):\n",
    "        video_id = submission_part.video_id.iloc[i]\n",
    "        agent_id = submission_part.agent_id.iloc[i]\n",
    "        target_id = submission_part.target_id.iloc[i]\n",
    "        \n",
    "        if (video_id != stop_video_id[i]) or (agent_id != stop_agent_id[i]) or (target_id != stop_target_id[i]):\n",
    "            submission_part.stop_frame.iloc[i] = meta.video_frame.iloc[-1] + 1\n",
    "            \n",
    "    return submission_part\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:54:56.973644Z",
     "iopub.status.busy": "2025-12-03T17:54:56.973407Z",
     "iopub.status.idle": "2025-12-03T17:54:56.994042Z",
     "shell.execute_reply": "2025-12-03T17:54:56.993277Z"
    },
    "papermill": {
     "duration": 0.025278,
     "end_time": "2025-12-03T17:54:56.99523",
     "exception": false,
     "start_time": "2025-12-03T17:54:56.969952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_thresholds_optuna(y_true_dict, y_prob_dict, n_trials=100):\n",
    "    \"\"\"\n",
    "    Optimize thresholds for multiple actions using Optuna to maximize Macro F1.\n",
    "    \"\"\"\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    \n",
    "    action_names = list(y_true_dict.keys())\n",
    "    \n",
    "    def objective(trial):\n",
    "        f1_scores = []\n",
    "        for action in action_names:\n",
    "            y_t = y_true_dict[action]\n",
    "            y_p = y_prob_dict[action]\n",
    "            \n",
    "            # Suggest a float for this action\n",
    "            thresh = trial.suggest_float(action, 0.1, 0.8)\n",
    "            \n",
    "            # Calculate F1\n",
    "            score = f1_score(y_t, (y_p >= thresh).astype(int), zero_division=0)\n",
    "            f1_scores.append(score)\n",
    "            \n",
    "        # Return Macro F1\n",
    "        return np.mean(f1_scores)\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # Extract best thresholds\n",
    "    best_thresholds = study.best_params\n",
    "    \n",
    "    # Print result\n",
    "    print(f\"Optuna Best Macro F1: {study.best_value:.4f}\")\n",
    "    return best_thresholds\n",
    "\n",
    "def cross_validate_classifier(X, label, meta):\n",
    "    \"\"\"\n",
    "    Use ensemble of XGBoost + CatBoost for validation (consistent with training).\n",
    "    \"\"\"\n",
    "    y_true_dict = {}\n",
    "    y_prob_dict = {}\n",
    "    \n",
    "    print(\"  Collecting OOF predictions with ENSEMBLE (XGBoost + CatBoost)...\")\n",
    "    \n",
    "    for action in label.columns:\n",
    "        action_mask = ~label[action].isna().values\n",
    "        X_action = X[action_mask]\n",
    "        y_action = label[action][action_mask].values.astype(int)\n",
    "        groups_action = meta.video_id[action_mask]\n",
    "        \n",
    "        # Skip if not enough data\n",
    "        if len(np.unique(groups_action)) < 2 or len(np.unique(y_action)) < 2:\n",
    "            continue\n",
    "            \n",
    "        if not (y_action == 0).all():\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "                try:\n",
    "                    cv = StratifiedGroupKFold(n_splits=3)\n",
    "                    \n",
    "                    # --- THAY ĐỔI CHÍNH: ENSEMBLE XGBoost + CatBoost ---\n",
    "                    oof_probs_xgb = np.zeros(len(y_action))\n",
    "                    oof_probs_cat = np.zeros(len(y_action))\n",
    "                    \n",
    "                    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_action, y_action, groups_action)):\n",
    "                        X_train, X_val = X_action.iloc[train_idx], X_action.iloc[val_idx]\n",
    "                        y_train, y_val = y_action[train_idx], y_action[val_idx]\n",
    "                        \n",
    "                        # Train XGBoost\n",
    "                        model_xgb = DataSampler(clone(CFG.model), neg_pos_ratio=10.0)\n",
    "                        model_xgb.fit(X_train, y_train)\n",
    "                        oof_probs_xgb[val_idx] = model_xgb.predict_proba(X_val)[:, 1]\n",
    "                        \n",
    "                        # Train CatBoost\n",
    "                        cat_model = CatBoostClassifier(**CAT_PARAMS)\n",
    "                        model_cat = DataSampler(cat_model, neg_pos_ratio=10.0)\n",
    "                        model_cat.fit(X_train, y_train)\n",
    "                        oof_probs_cat[val_idx] = model_cat.predict_proba(X_val)[:, 1]\n",
    "                        \n",
    "                        del model_xgb, model_cat\n",
    "                        gc.collect()\n",
    "                    \n",
    "                    # ENSEMBLE: Trung bình của XGBoost và CatBoost\n",
    "                    oof_probs = (oof_probs_xgb + oof_probs_cat) / 2.0\n",
    "                    \n",
    "                    y_true_dict[action] = y_action\n",
    "                    y_prob_dict[action] = oof_probs\n",
    "                    # --- KẾT THÚC THAY ĐỔI ---\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in CV for {action}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    if not y_true_dict:\n",
    "        return {}\n",
    "\n",
    "    # Optimize using Optuna\n",
    "    print(\"  Optimizing thresholds with Optuna...\")\n",
    "    try:\n",
    "        best_thresholds = optimize_thresholds_optuna(y_true_dict, y_prob_dict, n_trials=100)\n",
    "        return best_thresholds\n",
    "    except Exception as e:\n",
    "        print(f\"Optuna optimization failed: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T17:54:57.001183Z",
     "iopub.status.busy": "2025-12-03T17:54:57.000926Z",
     "iopub.status.idle": "2025-12-03T17:54:57.00957Z",
     "shell.execute_reply": "2025-12-03T17:54:57.009036Z"
    },
    "papermill": {
     "duration": 0.012815,
     "end_time": "2025-12-03T17:54:57.010572",
     "exception": false,
     "start_time": "2025-12-03T17:54:56.997757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def robustify(submission, dataset, traintest, traintest_directory=None):\n",
    "    \"\"\"Ensure that the submission conforms to the three rules\"\"\"\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"{CFG.BASE_PATH}/{traintest}_tracking\"\n",
    "\n",
    "    # Rule 1: Ensure that start_frame >= stop_frame\n",
    "    old_submission = submission.copy()\n",
    "    submission = submission[submission.start_frame < submission.stop_frame]\n",
    "    if len(submission) != len(old_submission):\n",
    "        print(\"ERROR: Dropped frames with start >= stop\")\n",
    "    \n",
    "    # Rule 2: Avoid multiple predictions for the same frame from one agent/target pair\n",
    "    old_submission = submission.copy()\n",
    "    group_list = []\n",
    "    for _, group in submission.groupby(['video_id', 'agent_id', 'target_id']):\n",
    "        group = group.sort_values('start_frame')\n",
    "        mask = np.ones(len(group), dtype=bool)\n",
    "        last_stop_frame = 0\n",
    "        for i, (_, row) in enumerate(group.iterrows()):\n",
    "            if row['start_frame'] < last_stop_frame:\n",
    "                mask[i] = False\n",
    "            else:\n",
    "                last_stop_frame = row['stop_frame']\n",
    "        group_list.append(group[mask])\n",
    "    submission = pd.concat(group_list)\n",
    "    if len(submission) != len(old_submission):\n",
    "        print(\"ERROR: Dropped duplicate frames\")\n",
    "\n",
    "    # Rule 3: Submit something for every video\n",
    "    # Fill missing videos as in https://www.kaggle.com/code/ambrosm/mabe-validated-baseline-without-machine-learning\n",
    "    s_list = []\n",
    "    for idx, row in dataset.iterrows():\n",
    "        lab_id = row['lab_id']\n",
    "        if lab_id.startswith('MABe22'):\n",
    "            continue\n",
    "        video_id = row['video_id']\n",
    "        if (submission.video_id == video_id).any():\n",
    "            continue\n",
    "\n",
    "        if verbose: print(f\"Video {video_id} has no predictions.\")\n",
    "        \n",
    "        # Load video\n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "        vid = pd.read_parquet(path)\n",
    "    \n",
    "        # Determine the behaviors of this video\n",
    "        vid_behaviors = eval(row['behaviors_labeled'])\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n",
    "    \n",
    "        # Determine start_frame and stop_frame\n",
    "        start_frame = vid.video_frame.min()\n",
    "        stop_frame = vid.video_frame.max() + 1\n",
    "    \n",
    "        # Predict all possible actions as often as possible\n",
    "        for (agent, target), actions in vid_behaviors.groupby(['agent', 'target']):\n",
    "            batch_length = int(np.ceil((stop_frame - start_frame) / len(actions)))\n",
    "            for i, (_, action_row) in enumerate(actions.iterrows()):\n",
    "                batch_start = start_frame + i * batch_length\n",
    "                batch_stop = min(batch_start + batch_length, stop_frame)\n",
    "                s_list.append((video_id, agent, target, action_row['action'], batch_start, batch_stop))\n",
    "\n",
    "    if len(s_list) > 0:\n",
    "        submission = pd.concat([\n",
    "            submission,\n",
    "            pd.DataFrame(s_list, columns=['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])\n",
    "        ])\n",
    "        print(\"ERROR: Filled empty videos\")\n",
    "\n",
    "    submission = submission.reset_index(drop=True)\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-12-03T17:54:57.013075",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.train_path)\n",
    "train['n_mice'] = 4 - train[['mouse1_strain', 'mouse2_strain', 'mouse3_strain', 'mouse4_strain']].isna().sum(axis=1)\n",
    "train_without_mabe22 = train.query(\"~ lab_id.str.startswith('MABe22_')\")\n",
    "test = pd.read_csv(CFG.test_path)\n",
    "\n",
    "body_parts_tracked_list = list(np.unique(train.body_parts_tracked))\n",
    "# %%time\n",
    "f1_list = []\n",
    "submission_list = []\n",
    "for section in range(1, len(body_parts_tracked_list)): # skip index 0 (MABe22)\n",
    "    body_parts_tracked_str = body_parts_tracked_list[section]\n",
    "    try:\n",
    "        body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "        print(f\"{section}. Processing videos with {body_parts_tracked}\")\n",
    "        if len(body_parts_tracked) > 5:\n",
    "            body_parts_tracked = [b for b in body_parts_tracked if b not in drop_body_parts]\n",
    "    \n",
    "        # We read all training data which match the body parts tracked\n",
    "        train_subset = train[train.body_parts_tracked == body_parts_tracked_str]\n",
    "\n",
    "        _fps_lookup = (\n",
    "            train_subset[['video_id', 'frames_per_second']]\n",
    "            .drop_duplicates('video_id')\n",
    "            .set_index('video_id')['frames_per_second']\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "        single_mouse_list = []\n",
    "        single_mouse_label_list = []\n",
    "        single_mouse_meta_list = []\n",
    "\n",
    "        mouse_pair_list = []\n",
    "        mouse_pair_label_list = []\n",
    "        mouse_pair_meta_list = []   \n",
    "\n",
    "        for switch, data, meta, label in generate_mouse_data(train_subset, 'train'):\n",
    "            if switch == 'single':\n",
    "                single_mouse_list.append(data)\n",
    "                single_mouse_meta_list.append(meta)\n",
    "                single_mouse_label_list.append(label)\n",
    "            else:\n",
    "                mouse_pair_list.append(data)\n",
    "                mouse_pair_meta_list.append(meta)\n",
    "                mouse_pair_label_list.append(label)\n",
    "            \n",
    "            del data, meta, label\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        # Construct a binary classifier\n",
    "        binary_classifier = DataSampler(\n",
    "            clone(CFG.model),\n",
    "            neg_pos_ratio=10.0\n",
    "        )\n",
    "    \n",
    "        # Predict single-mouse actions\n",
    "        if len(single_mouse_list) > 0:\n",
    "            # Concatenate all batches\n",
    "            # The concatenation will generate label dataframes with missing values.\n",
    "\n",
    "            # Xử lý data của các con chuột theo fps\n",
    "            single_feats_parts = []\n",
    "            for data_i, meta_i in zip(single_mouse_list, single_mouse_meta_list):\n",
    "                fps_i = _fps_from_meta(meta_i, _fps_lookup)\n",
    "                X_i = transform_single(data_i, body_parts_tracked, fps_i).astype(np.float32)\n",
    "                single_feats_parts.append(X_i)\n",
    "                del fps_i, X_i\n",
    "            gc.collect()\n",
    "\n",
    "            X_tr = pd.concat(single_feats_parts, axis=0, ignore_index=True)\n",
    "            single_mouse_label = pd.concat(single_mouse_label_list, axis=0, ignore_index=True)\n",
    "            single_mouse_meta = pd.concat(single_mouse_meta_list, axis=0, ignore_index=True)\n",
    "            \n",
    "            del single_mouse_list, single_mouse_label_list, single_mouse_meta_list, single_feats_parts\n",
    "\n",
    "            if validate_or_submit == 'validate':\n",
    "                thresholds = cross_validate_classifier(X_tr, single_mouse_label, single_mouse_meta)\n",
    "                if thresholds:\n",
    "                    try:\n",
    "                        if os.path.exists('thresholds.json'):\n",
    "                            with open('thresholds.json', 'r') as f:\n",
    "                                all_thresh = json.load(f)\n",
    "                        else:\n",
    "                            all_thresh = {}\n",
    "\n",
    "                        section_key = str(body_parts_tracked_str)\n",
    "                        if section_key not in all_thresh:\n",
    "                            all_thresh[section_key] = {}\n",
    "\n",
    "                        all_thresh[section_key].update(thresholds)\n",
    "\n",
    "                        with open('thresholds.json', 'w') as f:\n",
    "                            json.dump(all_thresh, f, indent=4)\n",
    "                        print(\"  Saved thresholds to thresholds.json\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error saving thresholds: {e}\")\n",
    "            else:\n",
    "                train_and_save_models(body_parts_tracked_str, 'single', X_tr, single_mouse_label, single_mouse_meta, section)\n",
    "            del X_tr\n",
    "        # Predict mouse-pair actions\n",
    "        if len(mouse_pair_list) > 0:\n",
    "            # Concatenate all batches\n",
    "            # The concatenation will generate label dataframes with missing values.\n",
    "            \n",
    "            # Transform the coordinate representation into a distance representation for mouse_pair\n",
    "            # Use a subset of body_parts_tracked to conserve memory\n",
    "\n",
    "            pair_feats_parts = []\n",
    "            for data_i, meta_i in zip(mouse_pair_list, mouse_pair_meta_list):\n",
    "                fps_i = _fps_from_meta(meta_i, _fps_lookup)\n",
    "                X_i = transform_pair(data_i, body_parts_tracked, fps_i)\n",
    "                pair_feats_parts.append(X_i)\n",
    "                del X_i, fps_i\n",
    "            gc.collect()\n",
    "\n",
    "            X_tr = pd.concat(pair_feats_parts, axis=0, ignore_index=True)\n",
    "            mouse_pair_label = pd.concat(mouse_pair_label_list, axis=0, ignore_index=True)\n",
    "            mouse_pair_meta = pd.concat(mouse_pair_meta_list, axis=0, ignore_index=True)\n",
    "            del mouse_pair_list, mouse_pair_label_list, mouse_pair_meta_list, pair_feats_parts\n",
    "\n",
    "            if validate_or_submit == 'validate':\n",
    "                thresholds = cross_validate_classifier(X_tr, mouse_pair_label, mouse_pair_meta)\n",
    "                if thresholds:\n",
    "                    try:\n",
    "                        if os.path.exists('thresholds.json'):\n",
    "                            with open('thresholds.json', 'r') as f:\n",
    "                                all_thresh = json.load(f)\n",
    "                        else:\n",
    "                            all_thresh = {}\n",
    "\n",
    "                        section_key = str(body_parts_tracked_str)\n",
    "                        if section_key not in all_thresh:\n",
    "                            all_thresh[section_key] = {}\n",
    "\n",
    "                        all_thresh[section_key].update(thresholds)\n",
    "\n",
    "                        with open('thresholds.json', 'w') as f:\n",
    "                            json.dump(all_thresh, f, indent=4)\n",
    "                        print(\"  Saved thresholds to thresholds.json\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error saving thresholds: {e}\")\n",
    "            else:\n",
    "                train_and_save_models(body_parts_tracked_str, 'pair', X_tr, mouse_pair_label, mouse_pair_meta, section)\n",
    "            del X_tr\n",
    "        print(\"*\"*50)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f'***Exception*** {e}')\n",
    "    print()\n",
    "if validate_or_submit != 'validate':\n",
    "    if len(submission_list) > 0:\n",
    "        submission = pd.concat(submission_list)\n",
    "    else:\n",
    "        submission = pd.DataFrame(\n",
    "            dict(\n",
    "                video_id=438887472,\n",
    "                agent_id='mouse1',\n",
    "                target_id='self',\n",
    "                action='rear',\n",
    "                start_frame=278,\n",
    "                stop_frame=500\n",
    "            ), index=[44])\n",
    "    if validate_or_submit == 'submit':\n",
    "        submission_robust = robustify(submission, test, 'test')\n",
    "    submission_robust.index.name = 'row_id'\n",
    "    submission_robust.to_csv('submission.csv')\n",
    "    !head submission.csv"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "sourceId": 59156,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-03T17:54:50.04175",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
